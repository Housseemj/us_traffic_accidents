{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "165a2854-5a33-45a2-9b9f-dbb896c4e0ee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prep"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window \n",
    "\n",
    "#Chemins \n",
    "BRONZE_PATH = \"dbfs:/delta/bronze\" \n",
    "SILVER_PATH = \"dbfs:/delta/silver\"\n",
    "SILVER_TABLE = \"delta.`dbfs:/delta/silver`\"\n",
    "\n",
    "#Conf business \n",
    "BUSINESS_KEY = \"ID\"\n",
    "PARTITION_COL = \"accident_date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b5e301e-ce85-48eb-860b-66da953eff21",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Diag"
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = spark.read.format(\"delta\").load(BRONZE_PATH)\n",
    "print(\"Count bronze\", df_bronze.count())\n",
    "\n",
    "def simple_profile(df, n_unique_sample=50):\n",
    "    cols = df.columns\n",
    "    schema = [(c, str(df.schema[c].dataType)) for c in cols ]\n",
    "    nulls = [(c, df.filter(df[c].isNull()).count()) for c in cols]\n",
    "    distincts = [(c, df.select(c).distinct().count()) for c in cols]\n",
    "    profile = [(c, t, n, d) for (c,t),(c2,n),(c3,d) in zip(schema,nulls,distincts)]\n",
    "    return profile \n",
    "profile = simple_profile(df_bronze)\n",
    "for c,t,n,d in profile:\n",
    "    print(f\"{c} {t} {n} {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11c5956d-e94a-4426-843e-d00388d46a24",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fonctions"
    }
   },
   "outputs": [],
   "source": [
    "def parse_time(col, fmt=None):\n",
    "    if fmt:\n",
    "        return F.to_timestamp(F.col, fmt)\n",
    "    else:\n",
    "        return F.to_timestamp(F.col(col))\n",
    "\n",
    "def safe_cast(col, dtype):\n",
    "        return F.when(F.col(col).rlike(r'^-?\\d+(\\.\\d+)?$'), F.col(col).cast(dtype)).otherwise(F.lit(None))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41dc997b-3db5-4d9e-b82f-3ca022773832",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Silver Prep"
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = spark.read.format(\"delta\").load(\"dbfs:/delta/bronze\")\n",
    "\n",
    "df = df_bronze\n",
    "\n",
    "df = df.withColumn(\"accident_date\", F.to_date(F.col(\"Start_Time\")))\n",
    "\n",
    "df = df.withColumn(\"start_time_ts\", parse_time(\"Start_Time\")) \\\n",
    "       .withColumn(\"end_time_ts\", parse_time(\"End_Time\"))\n",
    "\n",
    "df = df.withColumn(\"start_lat\", safe_cast(\"Start_Lat\", DoubleType())) \\\n",
    "       .withColumn(\"start_lng\", safe_cast(\"Start_Lng\", DoubleType())) \\\n",
    "       .withColumn(\"end_lat\", safe_cast(\"End_Lat\", DoubleType())) \\\n",
    "       .withColumn(\"end_lng\", safe_cast(\"End_Lng\", DoubleType()))\n",
    "\n",
    "if \"Severity\" in df.columns:\n",
    "    df = df.withColumn(\"severity\", safe_cast(\"Severity\", IntegerType()))\n",
    "\n",
    "cols_keep = [\n",
    "      BUSINESS_KEY if BUSINESS_KEY in df.columns else F.monotonically_increasing_id().alias(\"gen_id\"),\n",
    "      \"start_time_ts\",\"end_time_ts\",\"accident_date\",\n",
    "      \"start_lat\",\"start_lng\",\"end_lat\",\"end_lng\",\n",
    "      \"severity\",\"City\",\"State\",\"Zipcode\",\"Country\",\"Weather_Condition\"\n",
    "]\n",
    "\n",
    "select_cols = []\n",
    "for c in [\"ID\",\"start_time_ts\",\"end_time_ts\",\"accident_date\",\"start_lat\",\"start_lng\",\"end_lat\",\"end_lng\",\"severity\",\"City\",\"State\",\"Zipcode\",\"Country\",\"Weather_Condition\"]:\n",
    "    if c in df.columns:\n",
    "        select_cols.append(F.col(c))\n",
    "\n",
    "if BUSINESS_KEY not in df.columns:\n",
    "    df = df.withColumn(\"ID\", F.sha2(F.concat_ws(\"||\", F.col(\"start_time_ts\").cast(\"string\"), F.col(\"start_lat\").cast(\"string\"), F.col(\"start_lng\").cast(\"string\")),256))\n",
    "\n",
    "final_cols = [\"ID\",\"start_time_ts\",\"end_time_ts\",\"accident_date\",\"start_lat\",\"start_lng\",\"severity\",\"City\",\"State\",\"Zipcode\",\"Country\",\"Weather_Condition\"]\n",
    "final_cols = [c for c in final_cols if c in df.columns]\n",
    "df_silver_prep = df.select(*final_cols)\n",
    "\n",
    "display(df_silver_prep.limit(50))\n",
    "print(\"Count rows candidate silver:\", df_silver_prep.count())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00fe781d-8db1-40dc-a008-705c07b23c9f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dedup"
    }
   },
   "outputs": [],
   "source": [
    "if \"_ingest_timestamp\" in df.columns:\n",
    "    w = Window.partitionBy(\"ID\").orderBy(F.col(\"_ingest_timestamp\").desc())\n",
    "else:\n",
    "    w = Window.partitionBy(\"ID\").orderBy(F.col(\"start_time_ts\").desc_nulls_last())\n",
    "\n",
    "df_dedup = df_silver_prep.withColumn(\"rn\", F.row_number().over(w)).filter(F.col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "print(\"Count after dedup:\", df_dedup.count())\n",
    "display(df_dedup.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74af765e-cfe0-4d57-8640-617722eda9b3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Missing Values"
    }
   },
   "outputs": [],
   "source": [
    "df_q = df_dedup.filter((F.col(\"start_lat\").isNotNull())& F.col(\"start_lng\").isNotNull())\n",
    "\n",
    "if \"end_time_ts\" in df_q.columns :\n",
    "    df_q = df_q.withColumn(\"trip_duration_sec\", F.unix_timestamp(\"end_time_ts\")- F.unix_timestamp(\"start_time_ts\"))\n",
    "    df_q = df_q.filter( (F.col(\"trip_duration_sec\").isNull()) | ((F.col(\"trip_duration_sec\") >= 0) & (F.col(\"trip_duration_sec\") < 86400)) )\n",
    "\n",
    "if \"severity\" in df_q.columns:\n",
    "    df_q = df_q.withColumn(\"severity\", F.when((F.col(\"severity\") >= 1) & (F.col(\"severity\") <= 5),F.col(\"severity\")).otherwise(None))\n",
    "\n",
    "for c in [\"City\",\"State\",\"Zipcode\",\"Country\"]:\n",
    "    if c in df_q.columns:\n",
    "        df_q = df_q.withColumn(c, F.coalesce(F.col(c), F.lit(\"Unknown\")))\n",
    "\n",
    "print(\"Count after Q rules:\", df_q.count())\n",
    "display(df_q.limit(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a9a7cc1-495d-47d2-82de-bde52bbc6be6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ingest in Silver"
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df_q\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .partitionBy(\"accident_date\")\n",
    "    .save(SILVER_PATH)\n",
    ")\n",
    "\n",
    "print(\"Silver written to:\", SILVER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "af21ef82-1891-4545-92b7-14d4b91c4c14",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check"
    }
   },
   "outputs": [],
   "source": [
    "df_silver = spark.read.format(\"delta\").load(\"dbfs:/delta/silver\")\n",
    "display(df_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f2662c5-0206-474d-96d4-b05c559291da",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "MERGE"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "source_df = df_q\n",
    "\n",
    "TEMP_BATCH = \"dbfs:/delta/batches/temp_batch\"\n",
    "source_df.write.format(\"delta\").mode(\"overwrite\").save(TEMP_BATCH)\n",
    "\n",
    "delta_silver = DeltaTable.forPath(spark, SILVER_PATH)\n",
    "source = spark.read.format(\"delta\").load(TEMP_BATCH)\n",
    "\n",
    "(\n",
    "  delta_silver.alias(\"t\")\n",
    "  .merge(\n",
    "    source.alias(\"s\"), \"t.ID = s.ID\"\n",
    "  )\n",
    "  .whenMatchedUpdateAll()\n",
    "  .whenNotMatchedInsertAll()\n",
    "  .execute()\n",
    "  ) \n",
    "\n",
    "print(\"Merge completed\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce07a02c-1f8f-45cc-961f-57cc4d3dda46",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Late Data"
    }
   },
   "outputs": [],
   "source": [
    "def reprocess_partition(partition_date_str, batch_df):\n",
    "\n",
    "    temp = f\"dbfs:/delta/batches/reprocess_{partition_date_str}\"\n",
    "    batch_df.write.format(\"delta\").mode(\"overwrite\").save(temp)\n",
    "\n",
    "    delta_silver = DeltaTable.forPath(spark, SILVER_PATH)\n",
    "    src = spark.read.format(\"delta\").load(temp)\n",
    "    (\n",
    "      delta_silver.alias(\"t\")\n",
    "      .merge(\n",
    "        src.alias(\"s\"), \"t.ID = s.ID\"\n",
    "      )\n",
    "      .whenMatchedUpdateAll()\n",
    "      .whenNotMatchedInsertAll()\n",
    "      .execute()\n",
    "      ) \n",
    "\n",
    "    print(f\"Reprocess completed for {partition_date_str}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef36066a-34d9-4da0-97d8-6c179ccc059f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data quality Check"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "Checks = []\n",
    "\n",
    "df_s = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "\n",
    "Checks.append((\"row_count\", df_s.count()>0))\n",
    "\n",
    "crit_cols = [\"ID\",\"start_time_ts\",\"start_lat\",\"start_lng\",]\n",
    "null_checks = {c: df_s.filter(F.col(c).isNull()).count() for c in crit_cols}\n",
    "for c, n in null_checks.items():\n",
    "    Checks.append((f\"null_{c}\", n == 0))  \n",
    "\n",
    "if \"severity\" in df_s.columns:\n",
    "    invalid_sev = df_s.filter((F.col(\"severity\").isNotNull()) & ~((F.col(\"severity\")>=1)&(F.col(\"severity\")<=5))).count()\n",
    "    Checks.append((\"severity_valid\", invalid_sev==0))\n",
    "\n",
    "for name, ok in Checks:\n",
    "    print(f\"{name} -> {'OK' if ok else 'FAILED'}\")\n",
    "\n",
    "dq = spark.createDataFrame([ (datetime.now().isoformat(), \"silver_quality\", all([ok for _,ok in Checks])) ],\n",
    "                           [\"check_time\",\"check\",\"status\"])\n",
    "dq.write.format(\"delta\").mode(\"append\").save(\"dbfs:/delta/quality_checks\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_transform_step",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
